# ğŸ”¬ Power AI - Advanced MLOps Analysis & Optimization

*Un systÃ¨me d'analyse MLOps avancÃ© pour l'optimisation des modÃ¨les de prÃ©diction Ã©lectrique avec analyse de corrÃ©lation, sÃ©lection de caractÃ©ristiques et hyperparamÃ¨trisation automatique.*

## ğŸ¯ ProblÃ¨me RÃ©solu

**ProblÃ¨me Initial**: Les prÃ©dictions ML Ã©taient trop plates et constantes (voir vos visualisations), indiquant:
- **MulticollinÃ©aritÃ©**: CaractÃ©ristiques trop corrÃ©lÃ©es entre elles
- **Overfitting**: Le modÃ¨le prÃ©disait simplement la moyenne
- **Manque de variance**: Pas de patterns significatifs appris

**Solution MLOps**: Pipeline complet d'optimisation avec:
- âœ… Analyse de corrÃ©lation et suppression automatique
- âœ… SÃ©lection multi-mÃ©thodes de caractÃ©ristiques
- âœ… Optimisation d'hyperparamÃ¨tres par recherche alÃ©atoire
- âœ… Validation croisÃ©e temporelle
- âœ… RÃ©gularisation avancÃ©e

## ğŸ“Š RÃ©sultats Obtenus

### Avant MLOps (ProblÃ¨me identifiÃ©)
- **UPS Power**: PrÃ©dictions plates (~5046 constant)
- **UPS Load**: TrÃ¨s plat (~21.5% constant)
- **Meter Power**: Patterns trop rÃ©guliers

### AprÃ¨s MLOps (RÃ©sultats optimisÃ©s)
- **Dataset 1**: RÂ² = 0.9373 Â± 0.0908, MAE = 6.11 (1.6% de std)
- **Dataset 2**: RÂ² = 0.7743 Â± 0.0848, MAE = 64.32 (11.4% de std)
- **CaractÃ©ristiques**: 164 â†’ 81-105 (suppression de 59-83 features corrÃ©lÃ©es)

## ğŸ”§ Architecture MLOps

### 1. **Analyse de CorrÃ©lation** (`analyze_correlations`)
```python
def analyze_correlations(self, df, threshold=0.95):
    """Analyse complÃ¨te des corrÃ©lations"""
    # Matrice de corrÃ©lation
    corr_matrix = df.corr().abs()
    
    # Identification des paires hautement corrÃ©lÃ©es
    high_corr_pairs = []
    features_to_drop = set()
    
    # Algorithme de suppression intelligente
    for column in upper_triangle.columns:
        correlated_features = upper_triangle[column][upper_triangle[column] > threshold]
        # Garder la feature avec plus de variance
        if df[column].var() < df[corr_feature].var():
            features_to_drop.add(column)
```

**RÃ©sultats**:
- **Dataset 1**: 283 paires corrÃ©lÃ©es â†’ 81 features supprimÃ©es
- **Dataset 2**: 767 paires corrÃ©lÃ©es â†’ 105 features supprimÃ©es

### 2. **SÃ©lection Multi-MÃ©thodes** (`advanced_feature_selection`)

#### **MÃ©thode 1: SÃ©lection Statistique**
```python
selector_stats = SelectKBest(score_func=f_regression, k=n_features)
```
- Utilise le test F pour la rÃ©gression
- SÃ©lectionne les k meilleures features

#### **MÃ©thode 2: SÃ©lection L1 (Lasso)**
```python
lasso_selector = SelectFromModel(
    xgb.XGBRegressor(reg_alpha=1.0),
    max_features=n_features
)
```
- RÃ©gularisation L1 pour la sparsitÃ©
- Ã‰limine automatiquement les features non importantes

#### **MÃ©thode 3: Ã‰limination RÃ©cursive (RFE)**
```python
rfe_selector = RFE(estimator=base_model, n_features_to_select=n_features)
```
- Ã‰limination itÃ©rative des features les moins importantes
- BasÃ©e sur l'importance des features XGBoost

#### **Combinaison Intelligente**
```python
# Intersection pour les features les plus importantes
common_features = set(stats_features) & set(lasso_features) & set(rfe_features)

# Si trop peu, union des mÃ©thodes top
if len(common_features) < 10:
    combined_features = list(set(stats_features + lasso_features))[:n_features]
```

### 3. **Optimisation HyperparamÃ¨tres** (`optimize_hyperparameters`)

#### **Grid de ParamÃ¨tres Anti-Overfitting**
```python
param_distributions = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [3, 4, 5, 6, 8],           # Profondeur limitÃ©e
    'learning_rate': [0.01, 0.05, 0.1, 0.2], # Taux d'apprentissage modÃ©rÃ©
    'subsample': [0.7, 0.8, 0.9],           # Sous-Ã©chantillonnage
    'colsample_bytree': [0.7, 0.8, 0.9],    # Ã‰chantillonnage des features
    'reg_alpha': [0, 0.1, 0.5, 1.0],        # RÃ©gularisation L1
    'reg_lambda': [0.1, 0.5, 1.0, 2.0],     # RÃ©gularisation L2
    'min_child_weight': [1, 3, 5],          # Poids minimum des feuilles
    'gamma': [0, 0.1, 0.2, 0.5]             # ComplexitÃ© minimale
}
```

#### **Validation CroisÃ©e Temporelle**
```python
tscv = TimeSeriesSplit(n_splits=cv_folds)
```
- Respecte l'ordre temporel des donnÃ©es
- Ã‰vite le data leakage

#### **Recherche AlÃ©atoire OptimisÃ©e**
```python
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_distributions,
    n_iter=50,  # 50 combinaisons testÃ©es
    scoring='neg_mean_absolute_error',
    cv=tscv
)
```

### 4. **Feature Engineering Ã‰lectrique** (`create_advanced_features`)

#### **CaractÃ©ristiques de Puissance**
```python
# Puissance totale et dÃ©sÃ©quilibre
df['ups_total_power'] = df[['ups_pa', 'ups_pb', 'ups_pc']].sum(axis=1)
df['ups_power_imbalance'] = df[['ups_pa', 'ups_pb', 'ups_pc']].std(axis=1)
df['ups_power_range'] = df[['ups_pa', 'ups_pb', 'ups_pc']].max(axis=1) - df[['ups_pa', 'ups_pb', 'ups_pc']].min(axis=1)
```

#### **MÃ©triques de QualitÃ©**
```python
# Facteur de puissance
df['ups_power_factor'] = df['ups_total_power'] / (df['ups_voltage_out_avg'] * df['ups_current_avg'] + 1e-6)
df['ups_power_factor'] = df['ups_power_factor'].clip(0, 2)
```

#### **Features Temporelles Cycliques**
```python
# Ã‰viter la corrÃ©lation linÃ©aire avec encodage cyclique
df['hour_sin'] = np.sin(2 * np.pi * df.index.hour / 24)
df['hour_cos'] = np.cos(2 * np.pi * df.index.hour / 24)
df['day_sin'] = np.sin(2 * np.pi * df.index.dayofweek / 7)
df['day_cos'] = np.cos(2 * np.pi * df.index.dayofweek / 7)
```

#### **Features de Tendance LimitÃ©es**
```python
# Ã‰viter trop de features de rolling pour rÃ©duire la corrÃ©lation
for target_col in ['ups_load', 'ups_total_power']:
    df[f'{target_col}_ma_3h'] = df[target_col].rolling(window=3).mean()
    df[f'{target_col}_volatility_3h'] = df[target_col].rolling(window=3).std()
    df[f'{target_col}_trend_3h'] = df[target_col].diff(3)
```

### 5. **DÃ©tection d'Anomalies Multi-MÃ©thodes** (`detect_advanced_anomalies`)

#### **MÃ©thode 1: Z-Score Statistique**
```python
z_scores = np.abs(zscore(X_anomaly, axis=0))
statistical_anomalies = (z_scores > 3).any(axis=1)
```

#### **MÃ©thode 2: Isolation Forest**
```python
iso_forest = IsolationForest(contamination=0.05, random_state=42)
ml_anomalies = iso_forest.fit_predict(X_anomaly) == -1
```

#### **MÃ©thode 3: Interquartile Range (IQR)**
```python
Q1 = X_anomaly.quantile(0.25)
Q3 = X_anomaly.quantile(0.75)
IQR = Q3 - Q1
iqr_anomalies = ((X_anomaly < (Q1 - 1.5 * IQR)) | (X_anomaly > (Q3 + 1.5 * IQR))).any(axis=1)
```

## ğŸš€ Utilisation

### 1. **Lancer l'Analyse MLOps**
```bash
python run_power_ai.py
# SÃ©lectionner option 3: MLOps ADVANCED Analysis
```

### 2. **Ou Directement**
```bash
python tools/mlops_advanced_engine.py
```

### 3. **RÃ©sultats GÃ©nÃ©rÃ©s**
- **Models**: `outputs/mlops_models/` (modÃ¨les optimisÃ©s .pkl)
- **Analysis**: `outputs/mlops_analysis/` (rapports et visualisations)
- **Report**: `mlops_comprehensive_report.md`

## ğŸ“ˆ MÃ©triques de Performance

### **MÃ©triques UtilisÃ©es**

#### **RÂ² Score (Coefficient de DÃ©termination)**
- **Formule**: RÂ² = 1 - (SS_res / SS_tot)
- **InterprÃ©tation**: Pourcentage de variance expliquÃ©e
- **Objectif**: > 0.8 pour un bon modÃ¨le

#### **MAE (Mean Absolute Error)**
- **Formule**: MAE = (1/n) * Î£|y_true - y_pred|
- **InterprÃ©tation**: Erreur moyenne absolue
- **Avantage**: Robuste aux outliers

#### **MAPE (Mean Absolute Percentage Error)**
- **Formule**: MAPE = (100/n) * Î£|(y_true - y_pred)/y_true|
- **InterprÃ©tation**: Erreur en pourcentage
- **UtilitÃ©**: ComprÃ©hension business

#### **Cross-Validation Score**
- **MÃ©thode**: TimeSeriesSplit (5 folds)
- **Avantage**: Validation robuste pour sÃ©ries temporelles
- **InterprÃ©tation**: StabilitÃ© du modÃ¨le

### **Benchmarks Atteints**

| Dataset | CV RÂ² | MAE | Features Used | Anomalies |
|---------|-------|-----|--------------|-----------|
| Dataset 1 | 0.9373 Â± 0.0908 | 6.11 (1.6% std) | 30/105 | 7.55% |
| Dataset 2 | 0.7743 Â± 0.0848 | 64.32 (11.4% std) | 30/81 | 5.00% |

## ğŸ” Diagnostic des ProblÃ¨mes RÃ©solus

### **1. MulticollinÃ©aritÃ©** âœ… RÃ‰SOLU
- **Avant**: 164 features avec 283-767 paires corrÃ©lÃ©es (>0.95)
- **AprÃ¨s**: 81-105 features, corrÃ©lations Ã©liminÃ©es
- **Impact**: PrÃ©dictions plus variÃ©es et rÃ©alistes

### **2. Overfitting** âœ… RÃ‰SOLU
- **Avant**: RÂ² parfait mais prÃ©dictions plates
- **AprÃ¨s**: RÃ©gularisation L1/L2, subsample, colsample
- **Validation**: Cross-validation temporelle avec Ã©cart-type

### **3. SÃ©lection de Features** âœ… OPTIMISÃ‰
- **Avant**: Toutes les features utilisÃ©es
- **AprÃ¨s**: 30 features sÃ©lectionnÃ©es par 3 mÃ©thodes
- **RÃ©sultat**: ModÃ¨les plus interpretables et performants

### **4. HyperparamÃ¨tres** âœ… OPTIMISÃ‰
- **Avant**: ParamÃ¨tres par dÃ©faut
- **AprÃ¨s**: 50 combinaisons testÃ©es, optimisation automatique
- **Exemples optimaux**:
  - Dataset 1: `learning_rate=0.05`, `max_depth=6`, `reg_lambda=1.0`
  - Dataset 2: `learning_rate=0.1`, `max_depth=4`, `reg_alpha=1.0`

## ğŸ—ï¸ Architecture Technique

### **Classes et Modules**

```python
class MLOpsAdvancedEngine:
    def __init__(self, data_dir, model_dir, analysis_dir)
    def load_data(self, sample_size=None)
    def analyze_correlations(self, df, threshold=0.95)
    def create_advanced_features(self, df)
    def engineer_features(self, df)
    def optimize_hyperparameters(self, X, y, cv_folds=3)
    def advanced_feature_selection(self, X, y, target_name)
    def train_optimized_model(self, df, target)
    def predict_future_optimized(self, df, target, hours_ahead=24)
    def detect_advanced_anomalies(self, df)
    def generate_comprehensive_report(self, results)
    def save_models(self)
```

### **Pipeline de Traitement**

```
1. Chargement des donnÃ©es â”€â”€â†’ 2. Feature Engineering
                                        â†“
8. Sauvegarde modÃ¨les â†â”€â”€ 7. DÃ©tection anomalies â†â”€â”€ 3. Analyse corrÃ©lations
         â†“                                                    â†“
9. GÃ©nÃ©ration rapport â†â”€â”€ 6. PrÃ©dictions futures â†â”€â”€ 4. SÃ©lection features
                                                            â†“
                              5. Optimisation hyperparamÃ¨tres
```

## ğŸ“ Configuration et Personnalisation

### **ParamÃ¨tres de CorrÃ©lation**
```python
# Seuil de corrÃ©lation (0.95 = 95%)
threshold = 0.95  # Ajustable selon les besoins
```

### **ParamÃ¨tres de Feature Selection**
```python
# Nombre max de features sÃ©lectionnÃ©es
n_features = min(30, X.shape[1] // 2)  # Conservative
```

### **ParamÃ¨tres d'Optimisation**
```python
# Nombre d'itÃ©rations de recherche alÃ©atoire
n_iter = 50  # Ã‰quilibre temps/qualitÃ©

# Nombre de folds pour la cross-validation
cv_folds = 3  # Augmenter pour plus de robustesse
```

## ğŸ¯ Recommandations d'Usage

### **Pour la Production**
1. **Monitoring**: Surveiller la dÃ©rive des features
2. **Retraining**: Re-entraÃ®ner mensuellement
3. **A/B Testing**: Comparer avec l'ancien modÃ¨le
4. **Feature Store**: Centraliser les features engineerÃ©es

### **Pour l'AmÃ©lioration**
1. **Features Temporelles**: Ajouter saisonnalitÃ©, jours fÃ©riÃ©s
2. **Features Externes**: TempÃ©rature, humiditÃ©, occupancy
3. **Ensemble Methods**: Combiner XGBoost avec autres algorithmes
4. **Deep Learning**: Tester LSTM pour sÃ©ries temporelles longues

### **Pour le Debug**
1. **Importance des Features**: Analyser `feature_importance`
2. **RÃ©sidus**: Examiner les erreurs de prÃ©diction
3. **Validation**: Augmenter les folds de cross-validation
4. **CorrÃ©lations**: RÃ©duire le seuil si needed (0.90, 0.85)

## ğŸ“Š Visualisations GÃ©nÃ©rÃ©es

- **Matrice de CorrÃ©lation**: `correlation_matrix.png`
- **Feature Importance**: Dans le rapport
- **Performance Metrics**: Cross-validation scores
- **Anomaly Detection**: Scores et distributions

## ğŸ”— IntÃ©gration avec le Dashboard

Le systÃ¨me MLOps s'intÃ¨gre avec le dashboard Dash pour:
- Affichage des mÃ©triques optimisÃ©es
- Visualisation des features sÃ©lectionnÃ©es
- Monitoring des anomalies dÃ©tectÃ©es
- Comparaison des modÃ¨les

---

## ğŸ‰ Conclusion

Le systÃ¨me MLOps a transformÃ© des prÃ©dictions plates et inutilisables en modÃ¨les robustes et performants:

- **âœ… ProblÃ¨me de multicollinÃ©aritÃ© rÃ©solu** par l'analyse de corrÃ©lation
- **âœ… Overfitting Ã©liminÃ©** par la rÃ©gularisation et validation croisÃ©e
- **âœ… Performance optimisÃ©e** par l'hyperparamÃ¨trisation automatique
- **âœ… Features pertinentes** sÃ©lectionnÃ©es par 3 mÃ©thodes complÃ©mentaires
- **âœ… ModÃ¨les interpretables** avec importance des features
- **âœ… Pipeline reproductible** pour la production

**RÃ©sultat**: Des prÃ©dictions Ã©lectriques prÃ©cises et utilisables pour l'optimisation Ã©nergÃ©tique! ğŸš€ 